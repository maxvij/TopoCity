{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from  geopy.geocoders import Nominatim\n",
    "import requests\n",
    "import json\n",
    "import qwikidata\n",
    "import qwikidata.sparql\n",
    "\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import display_html\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#os.chdir('/Users/karlijnrozestraten/Documents/Development.nosync')\n",
    "\n",
    "#all_files = os.listdir('Woonplaatsen_NL')\n",
    "\n",
    "#li = []\n",
    "\n",
    "#for filename in all_files:\n",
    "  #  df = pd.read_csv('Woonplaatsen_NL/' + filename,sep=';', header=1)\n",
    "   # df.drop(df.tail(1).index,inplace=True)\n",
    "    #li.append(df)\n",
    "\n",
    "#city_names = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "#city_names.to_csv('city_names.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CBS city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from: https://opendata.cbs.nl/#/CBS/nl/dataset/84734NED/table\n",
    "# select alle places\n",
    "cities = pd.read_csv('Woonplaatsen_NL_complete.csv', sep=';')\n",
    "cities = cities.set_axis(['Woonplaatsen','Gemeente', 'Provincie','Landsdeel'], axis=1, inplace=False)\n",
    "cities['Provincie'] = cities['Provincie'].str.replace(' ','')\n",
    "cities.to_csv('cities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve information from Wikipedia, as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_places = ['Abbekerk','Basse', 'Bruinehaar', 'Barsingerhorn','Bloemendaal', 'Collendoorn', 'Groet','Driehuizen','Delden','Driewegen', 'Hem', 'Holten', 'Hoek', 'Lutten', 'Markelo', 'Ressen', 'Morra','Nieuwerkerk','Oostburg','Halfweg','Riel','Houwerzijl','Onstwedde', 'Welsum','Julianadorp','Koedijk','Noordeinde','Purmer',\n",
    "            'Schiphol','Schiphol-Rijk','Spaarndam','Wieringerwaard','Zwaag','Aalst','Doesburg','Ewijk','Hattem','Hattemerbroek',\n",
    "            'Heelweg','Herveld','Klarenbeek','Lienden','Ommeren','Ooij','Poederoijen','Steenenkamer','Vethuizen','Voorst','Voorthuizen',\n",
    "            'Wapenveld','Well','Westendorp','Wijnbergen','Zelhem','Abbega','Baard','Bornwird','Dokkum','Ee','Exmorra','Hallum',\n",
    "            'Hantum','Hemrik','Indijk','Midsland','Miedum','Oosthem','Siegerswoude','Sumar','Surhuizum','Teerns','Barendrecht',\n",
    "            'Katwijk','Stolwijk','Strijen','Strijensas','Zevenhuizen','Alphen','Baarle-Nassau','Bakel','Chaam','Doeveren',\n",
    "            'Gastel','Heeze','Herpt','Hoeven','Nispen','Noordhoek','Oisterwijk','Oosteind','Ulvenhout','Wernhout','Wouw','Zevenbergen',\n",
    "            'Aduard','Borgercompagnie','Finsterwolde','Jonkersvaart','Nieuwolda','Noordhorn','Oldenzijl','Oostwold','Oudezijl','Startenhuizen',\n",
    "            'Vierhuizen','Vlagtwedde','Wildervank']               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataframe for output\n",
    "results = pd.DataFrame(columns = ['Place','Link','Province','Coordinates', 'Population', 'Lattitude', 'Longitude'])\n",
    "\n",
    "\n",
    "# retrieve information from wikipedia page about provinces\n",
    "response = get('https://nl.wikipedia.org/wiki/Categorie:Lijsten_van_Nederlandse_plaatsen_naar_provincie')\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "data = html_soup.find('div',{'class':'mw-category'})\n",
    "\n",
    "# retrieve the correct links for the pages with all places per province\n",
    "links = data.findAll('a')\n",
    "provinces_links = []\n",
    "for link in links:\n",
    "    if link['href'][6:22] == 'Lijst_van_steden':\n",
    "        plink = 'https://nl.wikipedia.org'+link['href']\n",
    "        provinces_links.append(plink)\n",
    "# create a list of all the unique provinces to make iteration possible\n",
    "provinces = cities['Provincie'].unique()\n",
    "\n",
    "# start the iteration for each provinces\n",
    "for province in provinces:\n",
    "    item_list = []\n",
    "\n",
    "    # retrieve the overview page of the province\n",
    "    pl = re.compile('.*\\D({})$'.format(province))\n",
    "    for find_link in provinces_links:\n",
    "        m = pl.match(find_link)\n",
    "\n",
    "        if m:\n",
    "            province_page = get(find_link)\n",
    "            html_soup = BeautifulSoup(province_page.text, 'html.parser')        \n",
    "            # expend the iteration by looping for each place in a province. Create a list of the places in a province\n",
    "            places_in_province = cities.loc[cities['Provincie'] == province, 'Woonplaatsen']\n",
    "            for place in places_in_province:\n",
    "                if place not in no_places:\n",
    "\n",
    "                    # The format of the province Drenthe is slightly different then the other provinces. The approach differs slightly\n",
    "                    if province == 'Drenthe':\n",
    "                        # Only retrieve the elements where the information we need is in\n",
    "                        page_data = html_soup.findAll('table',{'class':''})\n",
    "                        tr_list = [tr.findAll('tr') for tr in page_data]\n",
    "                        place_link_list = [li.find('a') for item in tr_list for li in item if li.find('a')!= None]\n",
    "                        # Get the link for the place page in order to retrieve information about the place\n",
    "\n",
    "                        for place_link in place_link_list:\n",
    "                            if place_link != None:\n",
    "                                if place_link['href'][6:(len(place)+6)] == place:\n",
    "\n",
    "                                    placelink = 'https://nl.wikipedia.org'+place_link['href']\n",
    "\n",
    "                                    # Get access to the infobox on the pace\n",
    "                                    place_page = get(placelink)\n",
    "                                    html_soup = BeautifulSoup(place_page.text, 'html.parser')\n",
    "                                    place_data = html_soup.find('table',{'class':'infobox'})\n",
    "                                    # Retrieve the coordinate and population information\n",
    "                                    table_df = pd.read_html(str(place_data))[0]\n",
    "                                    co = table_df.loc[table_df[0] == 'Coördinaten',[1]].values[0]\n",
    "                                    pop = table_df[table_df[0].str.match(r'Inwoners.*')==True][1].values[0]\n",
    "\n",
    "                                    # Transform the coordinates to decimals\n",
    "                                    split = str(co).rsplit(',')\n",
    "                                    lat = split[0]\n",
    "                                    long = split[1]\n",
    "                                    lat_ = re.findall('\\d+', lat)\n",
    "                                    long_ = re.findall('\\d+',long)\n",
    "                                    lat_ = [int(c) for c in lat_]\n",
    "                                    long_ = [int(c) for c in long_]\n",
    "\n",
    "                                    if len(lat_) != 3:\n",
    "                                        lat_.append(0)\n",
    "                                    latdd = lat_[0] + (lat_[1]/60) + (lat_[2]/3600)\n",
    "\n",
    "                                    if len(long_) !=3:\n",
    "                                        long_.append(0)\n",
    "                                    longdd = long_[0] + (long_[1]/60) + (long_[2]/3600)\n",
    "                                    # Add the information to the dataframe\n",
    "                                    results.loc[len(results)] = [place, placelink, province, co, pop, latdd, longdd]\n",
    "                                           \n",
    "\n",
    "                    else:\n",
    "                        # Retrieve information form the province overview page\n",
    "                        page_data = html_soup.find('div',{'class':'mw-parser-output'})\n",
    "                        tr_list = page_data.findAll('tr')\n",
    "                        # Create a list with the places in the province and the link to it\n",
    "                        for item in tr_list:\n",
    "                            item_list.append(item.findAll('li'))   \n",
    "                        places_list = [li.find('a') for li_list in item_list for li in li_list if li != None]\n",
    "                        # Get the link for the place page in order to retrieve information about the place\n",
    "                        for place_link in places_list:\n",
    "                            if place_link != None and place_link['href'][6:] != 'Baarlo_(Zwartewaterland)':\n",
    "\n",
    "                                if place_link['href'][6:(len(place)+6)] == place:\n",
    "                                    placelink = 'https://nl.wikipedia.org'+place_link['href']\n",
    "\n",
    "                                    # Get access to the infobox on the pace\n",
    "                                    place_page = get(placelink)\n",
    "                                    html_soup = BeautifulSoup(place_page.text, 'html.parser')\n",
    "                                    place_data = html_soup.find('table',{'class':'infobox'})\n",
    "                                    # Retrieve the coordinate and population information\n",
    "                                    table_df = pd.read_html(str(place_data))[0]\n",
    "                                    co = table_df.loc[table_df[0] == 'Coördinaten',[1]].values[0]\n",
    "                                    pop = table_df[table_df[0].str.match(r'Inwoners.*')==True][1].values[0]\n",
    "\n",
    "                                    # Transform the coordinates to decimals\n",
    "                                    split = str(co).rsplit(',')\n",
    "                                    lat = split[0]\n",
    "                                    long = split[1]\n",
    "                                    lat_ = re.findall('\\d+', lat)\n",
    "                                    long_ = re.findall('\\d+',long)\n",
    "                                    lat_ = [int(c) for c in lat_]\n",
    "                                    long_ = [int(c) for c in long_]\n",
    "\n",
    "                                    if len(lat_) != 3:\n",
    "                                        lat_.append(0)\n",
    "                                    latdd = lat_[0] + (lat_[1]/60) + (lat_[2]/3600)\n",
    "\n",
    "                                    if len(long_) !=3:\n",
    "                                        long_.append(0)\n",
    "                                    longdd = long_[0] + (long_[1]/60) + (long_[2]/3600)\n",
    "                                    # Add the information to the dataframe\n",
    "                                    results.loc[len(results)] = [place, placelink, province, co, pop, latdd, longdd]\n",
    "                                    \n",
    "\n",
    "results.to_csv('results.csv')\n",
    "print(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After manual adjustments, read in complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_complete = pd.read_csv('cities_edit.csv', sep=';')\n",
    "# Drop the columns we don't need\n",
    "cities_complete = cities_complete.drop(columns = ['Unnamed: 0','L1','l2','L3','Lat','L1.1','L1.2','L.1.3','Long','Unnamed: 17'])\n",
    "# Drop places without population data\n",
    "cities_complete = cities_complete[cities_complete['Population'] != 'None']\n",
    "# Change values op column population into integers\n",
    "cities_complete['Population'] = cities_complete['Population'].astype(int)\n",
    "\n",
    "cities_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the rows were Lattitude and Longitude are empty\n",
    "missinglong_lat = cities_complete.loc[(cities_complete['Lattitude'].isnull()) & (cities_complete['Longitude'].isnull())]\n",
    "\n",
    "# For each coordinate in the new dataframe we will calculate the Longitude and Lattitude\n",
    "for coordinates in missinglong_lat['Coordinates']:\n",
    "    \n",
    "    # Get the real index of the row in the big df\n",
    "    index_co = missinglong_lat[missinglong_lat['Coordinates']== coordinates].index.values\n",
    "    place = missinglong_lat['Woonplaatsen'][index_co].values\n",
    "    real_index = cities_complete.loc[(cities_complete['Coordinates'] == coordinates) & (cities_complete.Woonplaatsen == place[0])].index.values\n",
    "    \n",
    "    # Calculate the coorinates to longitude and lattitude decimals\n",
    "    split = str(coordinates).rsplit(',')\n",
    "    lat = split[0]\n",
    "    long = split[1]\n",
    "    lat_ = re.findall('\\d+', lat)\n",
    "    long_ = re.findall('\\d+',long)\n",
    "    lat_ = [int(c) for c in lat_]\n",
    "    long_ = [int(c) for c in long_]\n",
    "    \n",
    "    if len(lat_) == 2:\n",
    "        lat_.append(0)\n",
    "    if len(lat_) == 1:\n",
    "        lat_ = lat_ + [0,0]\n",
    "    latdd = lat_[0] + (lat_[1]/60) + (lat_[2]/3600)\n",
    "\n",
    "    if len(long_) == 2:\n",
    "        long_.append(0)\n",
    "    if len(long_) == 1:\n",
    "        long_ = long_ + [0,0]\n",
    "    longdd = long_[0] + (long_[1]/60) + (long_[2]/3600)\n",
    "    \n",
    "    # Put the longitude and lattitude data into the big dataframe\n",
    "    cities_complete.at[index_co,'Lattitude'] = latdd\n",
    "    cities_complete.at[index_co,'Longitude'] = longdd\n",
    "\n",
    "# Write the complete dataframe to CSV.\n",
    "cities_complete.to_csv('cities_complete_clean.csv')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a selection of cities where the population is equal or higher than 20000 and write that to a new csv.\n",
    "cities_20k = cities_complete.loc[(cities_complete['Population']>=20000)]\n",
    "cities_20k.to_csv('cities_20k.csv')\n",
    "cities_20k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
